{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature detectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from orb_sift_detectors import extract_keypoints_and_descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs_path = os.path.join(os.path.dirname(os.path.dirname(os.getcwd())), \n",
    "                              'test_imgs')\n",
    "image_path_1 = os.path.join(test_imgs_path,'cam2_1.jpg')\n",
    "image_path_2 = os.path.join(test_imgs_path, 'cam1_1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORB detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kp1, d1), (kp2, d2) = extract_keypoints_and_descriptors(image_path_1,\n",
    "                                                         image_path_2,\n",
    "                                                         detector_type='ORB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv.drawKeypoints(cv.imread(image_path_1, cv.IMREAD_GRAYSCALE), \n",
    "                        kp1, \n",
    "                        None, \n",
    "                        color=(0,255,0), \n",
    "                        flags=cv.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "img2 = cv.drawKeypoints(cv.imread(image_path_2, cv.IMREAD_GRAYSCALE), \n",
    "                        kp1, \n",
    "                        None, \n",
    "                        color=(0,255,0), \n",
    "                        flags=cv.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIFT detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(kp1, d1), (kp2, d2) = extract_keypoints_and_descriptors(image_path_1,\n",
    "                                                         image_path_2,\n",
    "                                                         detector_type='SIFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = cv.drawKeypoints(cv.imread(image_path_1, cv.IMREAD_GRAYSCALE), \n",
    "                        kp1, \n",
    "                        None, \n",
    "                        color=(0,255,0), \n",
    "                        flags=cv.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)\n",
    "img2 = cv.drawKeypoints(cv.imread(image_path_2, cv.IMREAD_GRAYSCALE), \n",
    "                        kp1, \n",
    "                        None, \n",
    "                        color=(0,255,0), \n",
    "                        flags=cv.DrawMatchesFlags_DRAW_RICH_KEYPOINTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img2)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(img1)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SuperPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, SuperPointForKeypointDetection\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1 = Image.open(image_path_1).convert('RGB')\n",
    "image_2 = Image.open(image_path_2).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image_1, image_2]\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\n",
    "model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)  # The outputs contain the list of keypoint coordinates with their \n",
    "                           # respective score and description (a 256-long vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_imgs = [None, None]\n",
    "\n",
    "for i in range(len(images)):\n",
    "    image_mask = outputs.mask[i]\n",
    "    image_indices = torch.nonzero(image_mask).squeeze()\n",
    "    image_keypoints = outputs.keypoints[i][image_indices]\n",
    "    image_scores = outputs.scores[i][image_indices]\n",
    "    image_descriptors = outputs.descriptors[i][image_indices]\n",
    "    \n",
    "    image_np = np.transpose(inputs['pixel_values'][i], (1, 2, 0)).numpy()\n",
    "\n",
    "    # Ensure the image is contiguous and in uint8 format (0-255 range)\n",
    "    if image_np.max() <= 1.0:\n",
    "        image_np = (image_np * 255).astype(np.uint8)\n",
    "    else:\n",
    "        image_np = image_np.astype(np.uint8)\n",
    "\n",
    "    image_np = np.ascontiguousarray(image_np)\n",
    "    \n",
    "    for keypoint, score in zip(image_keypoints, image_scores):\n",
    "        keypoint_x, keypoint_y = int(keypoint[0].item()), int(keypoint[1].item())\n",
    "        color = (0, 0, 255)\n",
    "        image_np = cv.circle(image_np, (keypoint_x, keypoint_y), 2, color, thickness=-1)\n",
    "    \n",
    "    show_imgs[i] = image_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1_rgb = cv.cvtColor(show_imgs[0], cv.COLOR_BGR2RGB)\n",
    "image_2_rgb = cv.cvtColor(show_imgs[1], cv.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image_1_rgb)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(image_2_rgb)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1. Использовать SIFT не будем, т.к. скорость его работы низка, да и как видно, при матчинге будет очень много несоответствий.  \n",
    "2. ORB тоже показывает неудовлетворительные результаты, т.к. как видно он не отмечает все точки на объекте равномерно.  \n",
    "3. Метод SuperPoint (на основе нейронной сети) работает значительно лучше - равномерность точек по объекту. Вдобавок к алгоритму поиска ключевых точек SuperPoint существует алгоритм матчинга (от тех же исследователей/разработчиков) который является логичным продолжением этого - [SuperGlue](https://github.com/magicleap/SuperGluePretrainedNetwork/tree/master)\n",
    "\n",
    "Оставшиеся задачи:\n",
    "1. Попробовать другие детекторы на основе DL (D2Net, R2D2)\n",
    "2. Применить последовательно к SuperPoint алгоритм SuperGlue для матчинга точек\n",
    "3. Также остается идея для использования сегментации чтобы отделить сам объект от фона и уже только на нем искать характеристические точки"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack-sev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
